{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 04 DQN - Assignment\n",
    "\n",
    "\n",
    "In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as an input to the neural network. \n",
    "The output of the neural network represents the (estimated) Q-values of all possible actions. Using an argmax, we choose the action corresponding to the highest Q-value.\n",
    "\n",
    "\n",
    "To train the Q network, we sample a batch of stored experiences from the replay memory. An experience is a tuple of (state, action, reward, next_state).\n",
    "We input the state into the Q network and get the estimated Q-values. For the Q network to adjust the weights, it needs to have an idea of how accurate these predicted Q-values are.\n",
    "However, we do not know the target or actual value here as we are dealing with a reinforcement learning problem. The solution is to estimate the target value by using a second neural network, called the target network. This target network will take the next state as an input and predict the Q-values for all possible actions from that state. \n",
    "Now we can compute the labels $$y$$ to train the policy network: $$y = R(s, a) + \\gamma max_{a'}Q(s', a') - Q_{t-1}(s, a)$$\n",
    "The Q network can now be trained with the MSE loss. It's important to know that the target network is an exact copy of the policy network and the weights of the target network \n",
    "\n",
    "After a certain amount of Q-network updates, we copy its weights to the target network.\n",
    "\n",
    "For more detailed information: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "# Import Tensorflow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCar-V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "The agent (a car) is started at the bottom of a valley. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\n",
    "\n",
    "<img src=\"./NotebookImages/MountainCart.gif\">\n",
    "\n",
    "For a description of the statevector, the action space and the episode termination,have a look at:https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "\n",
    "- Implement a DQN to solve this environment.\n",
    "- Try to minimize the total number of steps per episode needed to reach the flag.\n",
    "- You are allowed to tweak the reward function. For example, giving an extra reward for getting closer to the flag.\n",
    "- Modify the DQN implementation into a deep SARSA implementation. Compare the deep SARSA to the DQN implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v2\n",
    "\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "For more information abou this environment see: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "<img src=\"./NotebookImages/LunarLander.gif\">\n",
    "\n",
    "- Implement a DQN to solve this environment. LunarLander-v2 defines \"solving\" as getting average reward of 200 over 100 consecutive trials. \n",
    "- Try to minimize the number of episodes it takes to solve the environment.\n",
    "- How would you tweak the reward function for the LunarLander to make a quicker descent.\n",
    "- Modify the DQN implementation into a deep SARSA implementation. Compare the deep SARSA to the DQN implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
